# -*- coding: utf-8 -*-
"""IBS Identification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q8DqfR_7o5o_knZHzPFHJ-NfU7z1o5aU

#Imports and Dataset processing
"""

import pandas as pd
import time
import numpy as np
import copy
from sympy import Symbol
from sympy.solvers import solve
pd.options.mode.chained_assignment = None
import csv
import warnings
import itertools
import seaborn as sns
import matplotlib
matplotlib.use('Agg')  # Use the non-interactive Agg backend
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import io, base64
import os

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.simplefilter('ignore')

"""Import Dataset

### Get Protected attributes data
"""

def _make_run_folder(prefix):
    """
    Create a unique folder in static/runs based on current time.
    Returns the relative path to that folder (e.g. 'runs/gcd_1677699482').
    """
    run_id = int(time.time())  # or use uuid.uuid4() for more uniqueness
    folder_name = f"{prefix}_{run_id}"
    # We'll store runs in 'static/runs/<prefix>_<timestamp>'
    run_folder = os.path.join("code\\static", "runs", folder_name)
    os.makedirs(run_folder, exist_ok=True)
    os.makedirs(os.path.join(run_folder, "K"), exist_ok=True)
    os.makedirs(os.path.join(run_folder, "Threshold"), exist_ok=True)
    # Return the relative path that Flask can use: 'runs/<prefix>_<timestamp>'
    return os.path.join("runs", folder_name)

def _save_plot(run_folder, file_name):
    save_path = os.path.join("code", "static", run_folder, file_name)
    with open(save_path, "w"):
      plt.savefig(save_path, bbox_inches="tight")
    plt.close()


def get_protected_attr_data(data, protected_attr, y_label):
    # Extract necessary columns first
    columns = protected_attr + [y_label]
    protected_data = data[columns]

    # Add a 'cnt' column set to 1 for counting
    protected_data['cnt'] = 1

    # Now you can safely perform operations that depend on 'cnt'
    cnt_protected_with_y = protected_data.groupby(protected_attr + [y_label]).size().reset_index(name='cnt')
    cnt_protected = protected_data.groupby(protected_attr).size().reset_index(name='cnt')

    return cnt_protected_with_y, cnt_protected, protected_data

# GET PARENTS and counts of regions
def get_parents_and_cnt_regions(data, protected_att, label):

    grrpby_data_lst = []
    for i in range(len(protected_att)):
        grp_names = copy.copy(protected_att)

        del grp_names[i]

        grp_names.append(label)
        temp_df = data.groupby(grp_names)['cnt'].count()
        grrpby_data_lst.append(temp_df)
    return grrpby_data_lst

def construct_hierarchy(protected_attributes):
    hierarchy = {}
    for i in range(1, len(protected_attributes) + 1):
        for combo in itertools.combinations(protected_attributes, i):
            hierarchy[len(hierarchy)] = list(combo)
    return hierarchy


def Calculate_ratio_rn(group_lst,parents, pos, neg):

    pos_cnt = 0
    neg_cnt = 0

    for i, df_groupby in enumerate(parents):

        temp_group_lst_pos = group_lst[:i] + group_lst[i+1:] + [1]
        temp_group_lst_neg = group_lst[:i] + group_lst[i+1:] + [0]


        # Assuming df_groupby is a DataFrame with appropriate indexing
        if tuple(temp_group_lst_pos) in df_groupby.keys():
          pos_cnt += df_groupby[tuple(temp_group_lst_pos)]

        if tuple(temp_group_lst_neg) in df_groupby.index:
            neg_cnt += df_groupby[tuple(temp_group_lst_neg)]


    # calculate pos, neg vals for ratio
    pos_val = pos_cnt - len(parents)* pos
    neg_val = neg_cnt - len(parents)* neg

    return -1 if neg_val == -1 or (neg_val == 0 and pos_val == 0) else 0 if pos_val == 0 or neg_val == 0 else pos_val / neg_val

"""## Calculate ratio"""

def compute_ratio(data, att_vals, protected_att, y_label):

    filtered_data = copy.deepcopy(data)
    # keep only the pos and neg count of the row
    for i in range(len(att_vals)):
      filtered_data = filtered_data[filtered_data[protected_att[i]] == att_vals[i]]

    # count the neg and pos examples
    total = filtered_data['cnt'].sum()
    pos = filtered_data[filtered_data[y_label] == 1]['cnt'].sum()
    neg = total - pos

    return pos, neg, total

"""Handle Imbalance"""

def handle_imbalance(ratio_rn, pos, neg, group_lst, need_pos, need_neg, imbalance_threshold):
    # Initialize imbalance status
    is_imbalance = 0

    # Determine imbalance based on ratio_rn
    if ratio_rn != -1:
        if ratio_rn == 0:
            # If ratio_rn is zero, check if there's a complete imbalance
            is_imbalance = 1 if neg == 0 and pos > 0 else 2 if pos == 0 and neg > 0 else 0
        else:
            # Calculate the actual ratio
            ratio_r = pos / neg
            # Determine if the imbalance exceeds the threshold
            if ratio_r - ratio_rn > imbalance_threshold:
                is_imbalance = 1
            elif ratio_rn - ratio_r > imbalance_threshold:
                is_imbalance = 2

    # Update need_neg or need_pos lists based on the imbalance status
    if is_imbalance == 1:
        if group_lst not in need_neg:
            need_neg.append(group_lst)
        else:
            print(f"{group_lst} is already in need_neg")
    elif is_imbalance == 2:
        if group_lst not in need_pos:
            need_pos.append(group_lst)
        else:
            print(f"{group_lst} is already in need_pos")

"""## IBS Algorithm

"""

def implicit_biased_set_identification(data, protected_attr, y_label, imbalance_threshold, k, run_folder, verbose=True):
  """Identifies sets with potential biases based on protected attributes."""
  start_time = time.time()
  hierarchy = construct_hierarchy(protected_attr)
  attrs = []

  for curr_protected_att in reversed(list(hierarchy.values())):

    # get data only with the currect protected attributes
    grpby_protected_y, grpby_protected, protected_data = get_protected_attr_data(data, curr_protected_att, y_label)

    #filter groups smaller than k
    grpby_protected = grpby_protected[grpby_protected['cnt'] > k]

    # count the groups of all the parents
    parents = get_parents_and_cnt_regions(protected_data, curr_protected_att, y_label)

    need_pos = []
    need_neg = []

    for index, row in grpby_protected.iterrows():
        group_lst = [row[att] for att in curr_protected_att]
        pos, neg, total = compute_ratio(grpby_protected_y, group_lst, curr_protected_att, y_label)
        is_imbalance = 0
        if(total > k ):
          ratio_rn = Calculate_ratio_rn(group_lst, parents, pos, neg)

          handle_imbalance(ratio_rn, pos, neg, group_lst, need_pos, need_neg, imbalance_threshold)

    if verbose:
      att_str = ' & '.join(curr_protected_att)
      attrs.append({
        'x_label': att_str,
        'need_pos_values': len(need_pos),
        'need_neg_values': len(need_neg),
        'need_total': len(need_neg) + len(need_pos)
      })


  end_time = time.time()
  excute_time = end_time - start_time

  if (len(attrs) != 0):
    max_update_vals = max(attrs, key=lambda x: x['need_total'])['need_total'] if len(attrs) != 0 else 0

    print('Subgroup with most required total updates is: ', max(attrs, key=lambda x: x['need_total'])['x_label'])

  attrs = sorted(attrs, key=lambda x: x["need_total"])

  # Create scatter plot
  plt.figure(figsize=(10, 6))
  sns.scatterplot(x=[att['x_label'] for att in attrs], y=[att['need_pos_values'] for att in attrs], color='blue', marker='^', label='Need Positive')
  sns.scatterplot(x=[att['x_label'] for att in attrs], y=[att['need_neg_values'] for att in attrs], color='red', marker='v', label='Need Negative')
  sns.scatterplot(x=[att['x_label'] for att in attrs], y=[att['need_total'] for att in attrs], color='green', marker='o', label='Need Total')

  plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))
  plt.xticks(rotation=90, ha="right")
  plt.xlabel("Subgroups of Protected Attributes")
  plt.ylabel("Need Pos / Need Neg")
  plt.title(f"Imbalance Identification Scatter Plot | K={k}m Imbalance Threshold={imbalance_threshold}")
  plt.legend()
  _save_plot(run_folder, f"{time.time()}-Imbalance Identification_K={k},Threshold={imbalance_threshold}.png")
  return excute_time, max_update_vals

"""# Run Dataset tests

"""

def run_tests(X, check_cols, y, run_folder, k_value, threshold_value):

  ks = []
  times = []
  thres = []
  maxs = []

  for i in range(0, int(k_value), 10):
    print("K: ", i, "imbalance threshold: 0.1")
    ks.append(i)
    excute_time, _ = implicit_biased_set_identification(X, check_cols, y, 0.1, i, os.path.join(run_folder, "K"), True)
    times.append(excute_time)

  for i in np.arange(0, float(threshold_value), 0.1):
    print("imbalance threshold: ", i, "K: 30")
    thres.append(i)
    _, max_update_vals = implicit_biased_set_identification(X, check_cols, y, i, 30, os.path.join(run_folder, "Threshold"), True)
    maxs.append(max_update_vals)

  plt.figure(figsize=(10, 6))
  sns.lineplot(x=ks, y=times, color='blue', marker='o')

  plt.xticks(rotation=90, ha="right")
  plt.xlabel("K Values")
  plt.ylabel("Execution Time")
  plt.title("Runtime as a function of K")
  plt.legend()
  _save_plot(run_folder, "Runtime as a function of K.png")

  plt.figure(figsize=(10, 6))
  sns.lineplot(x=thres, y=maxs, color='blue', marker='o')

  plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))
  plt.xticks(rotation=90, ha="right")
  plt.xlabel("Threshold")
  plt.ylabel("Max Total Updates Required")
  plt.title("Max Total Updates as a function of Imbalance Threshold")
  plt.legend()
  _save_plot(run_folder, "Max Total Updates as a function of Imbalance Threshold.png")

  return run_folder

"""### German Credit Data dataset"""
def run_gcd(selected_columns, k_value, threshold_value):
  run_folder = _make_run_folder("gcd")
  data_gcd = pd.read_csv(os.path.join(os.path.dirname(__file__), 'files', 'german_credit_data.csv'))

  # class is 1/2 instead of 0/1
  data_gcd['class'] = data_gcd['class'] - 1

  columns_gcd = ['Age', 'Sex', 'Job', 'Saving accounts']
  gcd_y = 'class'
  X_gcd = pd.DataFrame(data_gcd)

  return run_tests(X_gcd, selected_columns, gcd_y, run_folder, k_value, threshold_value)

"""### COMPAS Dataset

"""

def run_compas(selected_columns, k_value, threshold_value):
  data_compas = pd.read_csv(os.path.join(os.path.dirname(__file__), 'files', 'CleanAdult_numerical_cat.csv'))
  run_folder = _make_run_folder("compas")

  # protected attributes
  columns_compas = ['age', 'marital-status','relationship', 'race','gender', 'native-country']
  compas_y = 'income'
  return run_tests(data_compas, selected_columns, compas_y, run_folder, k_value, threshold_value)

"""### Stop, Question and Frisk

"""

def run_sqfd(selected_columns, k_value, threshold_value):
  data_sqfd = pd.read_csv(os.path.join(os.path.dirname(__file__), 'files', 'SQFD - CY 2017.csv'))
  run_folder = _make_run_folder("sqfd")

  # SUSPECT_ARRESTED_FLAG is Y/N instead of 0/1
  data_sqfd = data_sqfd.replace({"Y": 1, "N": 0})

  columns_sqfd = ['Age', 'Sex', 'Race']

  sqfd_y = 'SUSPECT_ARRESTED_FLAG'

  X_sqfd = pd.DataFrame(data_sqfd)
  return run_tests(X_sqfd, selected_columns, sqfd_y, run_folder, k_value, threshold_value)

"""### Default of credit card clients

"""

def run_ccc(selected_columns, k_value, threshold_value):
  data_ccc = pd.read_excel(os.path.join(os.path.dirname(__file__), 'files', 'default_of_credit_card_clients.xls'))
  run_folder = _make_run_folder("ccc")

  # X2: SEX, X3: EDUCATION, X4: MARRIAGE, X5: AGE
  columns_ccc = ['X2', 'X3', 'X4', 'X5']
  ccc_y = 'Y'
  X_ccc = pd.DataFrame(data_ccc)
  return run_tests(X_ccc, selected_columns, ccc_y, run_folder, k_value, threshold_value)